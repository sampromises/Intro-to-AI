'''Author: Sam KimAn implementation of the PureCFR algorithm to choose the best strategy innon-perfect imperfect games.'''###################################################################### Before implementing CFR, it will be helpful to take a look# at the helper methods in game.py. These are the methods you# need to interact with when you traverse the game tree in# CFR.#####################################################################import sysfrom game import Gameimport random # Needed to randomly sample# Debug printingDEBUG = 0####################################################### Add any classes or helper functions you want here####################################################### Helper debug function to print current strategydef print_strategy_profile(strategy_profile):    print "STRATEGY_PROFILE:"    for p in xrange(len(strategy_profile)):        print "\tplayer:", p        for i in xrange(len(strategy_profile[p])):            print "\t\tinfoset:", i            print "\t\t\taction probabilities:", strategy_profile[p][i]# Helper debug function to print current regretsdef print_regrets(regrets):    print "REGRETS:"    for p in xrange(len(regrets)):        print "\tplayer:", p        for i in xrange(len(regrets[p])):            print "\t\tinfoset:", i            print "\t\t\taction regrets:", regrets[p][i]# Helper debug function to print current strategy_sumsdef print_strategy_sums(strategy_sums):    print "STRATEGY_SUMS:"    for p in xrange(len(strategy_sums)):        print "\tplayer:", p        for i in xrange(len(strategy_sums[p])):            print "\t\tinfoset:", i            print "\t\t\taction strategy sums:", strategy_sums[p][i]# Initialize strategy_profile, regret totals, and strategy sums#   Initialize strategy to be uniform distribution,#   initialize regrets and strategy sums to be 0def initialize_data(g, strategy_profile, regrets, strategy_sums):    n = g.get_num_nodes()    # Initialize strategy_profile dicts    for node_id in xrange(n):        if (node_id not in g.node_nature_probabilities and not g.is_leaf(node_id)):            p = g.get_current_player(node_id)            i = g.node_infoset[node_id]            actions = g.get_num_actions_node(node_id)            uni_prob = float(1) / g.get_num_actions_infoset(p, i)            # Add player key            if p not in regrets:                strategy_profile[p] = {}                regrets[p] = {}                strategy_sums[p] = {}            # Add infoset key            if i not in strategy_profile[p]:                strategy_profile[p][i] = {}                regrets[p][i] = {}                strategy_sums[p][i] = {}            # Add action keys            for a in xrange(actions):                # Uniform probability values                strategy_profile[p][i][a] = uni_prob                # Initialize regrets                regrets[p][i][a] = 0                # Initialize strategy_sums                strategy_sums[p][i][a] = 0# Returns a uniform probabilitydef get_uniform_prob(actions):    n = len(actions)    return float(1) / n# Returns the index sampled in the given probability distributiondef sample_chance(probs):    sample = random.uniform(0, 1)    enum_probs = list(enumerate(probs))    enum_probs.sort(key = lambda x : x[1])    curr_prob = 0.0    for i in xrange(len(enum_probs)):        curr_prob += enum_probs[i][1]        if (sample <= curr_prob):            return enum_probs[i][0]    print "SHOULD NEVER REACH HERE... sampling returning a None probability..."    return None # Should never reach here# Returns the action (name) after sampling from a given player's infosetdef sample_strategies(strategy_profile, p, i):    # Get list of probabilities from dictionary    probs = []    for (key, val) in strategy_profile[p][i].iteritems():        probs.append(val)    # Enumerate probabilites for the return value    enum_probs = list(enumerate(probs))    enum_probs.sort(key = lambda x : x[1])    # Sample    sample = random.uniform(0, 1)    curr_prob = 0.0    for i in xrange(len(enum_probs)):        curr_prob += enum_probs[i][1]        if (sample <= curr_prob):            return enum_probs[i][0]# p^{T+1}(a) = max(R^T(a), 0) / sum from all a' in A of max(R^T(a'), 0)def regret_matching(strategy_profile, regrets, p, i, a_to_update):    # Regret of current action    if (regrets[p][i][a_to_update] < 0):        curr_action_regret = 0    else:        curr_action_regret = regrets[p][i][a_to_update]    # Sum all positive regrets    total_regret = 0.0    for a in regrets[p][i]:        if (regrets[p][i][a] < 0):            total_regret += 0        else:            total_regret += regrets[p][i][a]    # Edge case for div by 0    if (total_regret <= 0):        new_prob = get_uniform_prob(regrets[p][i])    else:        new_prob = curr_action_regret / float(total_regret)    strategy_profile[p][i][a_to_update] = new_probdef update_probabilities(strategy_profile, regrets, p, i, actions):    # Sum all positive regrets    total_regret = 0.0    for a in regrets[p][i]:        # total_regret += max(regrets[p][i][a], 0)        if (regrets[p][i][a] < 0):            total_regret += 0        else:            total_regret += regrets[p][i][a]    if (total_regret <= 0):        uni_prob = 1 / float(actions)        for a in xrange(actions):            strategy_profile[p][i][a] = uni_prob    else:        # Regret of current action        for a in xrange(actions):            # curr_action_regret = max(regrets[p][i][a], 0)            if (regrets[p][i][a] < 0):                curr_action_regret = 0            else:                curr_action_regret = regrets[p][i][a]            new_prob = curr_action_regret / float(total_regret)            strategy_profile[p][i][a] = new_probdef normalize_strategy_sums(strategy_profile, strategy_sums, num_iterations):    for p in xrange(len(strategy_sums)):        for i in xrange(len(strategy_sums[p])):            for a in xrange(len(strategy_sums[p][i])):                strategy_profile[p][i][a] = strategy_sums[p][i][a] / float(num_iterations)# g - game# s - strategy_profile# i - infosets# u - total received payoff from sequence of actions we tookdef pure_cfr(g, strategy_profile, regrets, strategy_sums, node_id, curr_player):    if DEBUG:        print "CURRENT NODE_ID ", node_id, "\n"        print_strategy_profile(strategy_profile)        print_regrets(regrets)        print_strategy_sums(strategy_sums)    # if (node is terminal)    if g.is_leaf(node_id):        utility = g.get_leaf_utility(node_id)        if (curr_player == 0):            # Util is for first player            return utility        else:            # Negative util for second player            return -utility    else:        # if (node is chance)        if node_id in g.node_nature_probabilities: # Or curr_player = -1            # sample action a from chance distribution            probabilities = g.node_nature_probabilities[node_id]            a = sample_chance(probabilities)            # return PureCFR(node.do_action(a), player)            next_node_id = g.get_child_id(node_id, a)            w = g.get_current_player(next_node_id)            return pure_cfr(g, strategy_profile, regrets, strategy_sums, next_node_id, curr_player)        elif g.get_current_player(node_id) is not curr_player:            p = g.get_current_player(node_id) # Updating probs for OTHER player            i = g.get_node_infoset(node_id)            actions = g.get_num_actions_node(node_id)            update_probabilities(strategy_profile, regrets, p, i, actions)            # sample action a            a = sample_strategies(strategy_profile, p, i)            # node.stored_strategy[a] += 1 // update the average strategy (normalize at the end)            strategy_sums[p][i][a] += 1            # return PureCFR(node.do_action(a), player)            next_node_id = g.get_child_id(node_id, a)            # next_player = g.get_current_player(next_node_id)            return pure_cfr(g, strategy_profile, regrets, strategy_sums, next_node_id, curr_player)        else:            p = curr_player            i = g.get_node_infoset(node_id)            actions = g.get_num_actions_node(node_id)            update_probabilities(strategy_profile, regrets, p, i, actions)            # sample action a            a = sample_strategies(strategy_profile, p, i)            sampled_next_node_id = g.get_child_id(node_id, a)            if (g.is_leaf(sampled_next_node_id)):                if (curr_player == 0):                    # Util is for first player                    sampled_action_value = g.get_leaf_utility(sampled_next_node_id)                else:                    # Negative util for second player                    sampled_action_value = -g.get_leaf_utility(sampled_next_node_id)                            else:                # sampled_next_player = g.get_current_player(sampled_next_node_id)                sampled_action_value = pure_cfr(g, strategy_profile, regrets, strategy_sums,                                             sampled_next_node_id, curr_player) # action_value[a]            # for each action in node:            #     action_value[action] = PureCFR(node.do_action(action), player)  // get the value for taking this action            # for each action in node:            #     node.regret[action] += action_value[action] - action_value[a] // update the regret            for action in xrange(actions):                next_node_id = g.get_child_id(node_id, action)                if g.is_leaf(next_node_id):                    if (curr_player == 0):                        # Util is for first player                        action_value = g.get_leaf_utility(next_node_id) # ? ? ?                    else:                        # Negative util for second player                        action_value = -g.get_leaf_utility(next_node_id) # ? ? ?                else:                    # next_player = g.get_current_player(next_node_id)                    action_value = pure_cfr(g, strategy_profile, regrets, strategy_sums, next_node_id, curr_player)                regrets[p][i][action] += action_value - sampled_action_value            # return action_value[a]            return sampled_action_value# game will be an instance of Game, which is defined in game.py# num_iterations is the number of PureCFR/MCCFR iterations you should perform.# An iteration of PureCFR/MCCFR is one traversal of the entire game tree for each playerdef solve_game(game, num_iterations):    # The goal of your algorithm is to fill    # strategy_profile with equilibrium strategies.    # strategy_profile[p][i][a] should return    # the probability of player p choosing the particular    # action a at information set i in the equilibrium    # you compute.    #    # An example set of values for a small game with 2    # information sets for each player would be:    #    strategy_profile[0][0] = [0.375, 0.625]    #    strategy_profile[0][1] = [1,0]    #    #    strategy_profile[1][0] = [0.508929, 0.491071]    #    strategy_profile[1][1] = [0.666667, 0.333333]    strategy_profile = {0:{}, 1:{}}    # Data structures to hold regrets and strategy sums    regrets = {}    strategy_sums = {}    # Initialize everything    initialize_data(game, strategy_profile, regrets, strategy_sums)    #######################    # Implement PureCFR or MCCFR in here    #######################    for i in xrange(num_iterations):        pure_cfr(game, strategy_profile, regrets, strategy_sums, game.get_root(), (i%2))    normalize_strategy_sums(strategy_profile, strategy_sums, num_iterations)    return strategy_profileif __name__ == "__main__":    # feel free to add any test code you want in here. It will not interfere with our testing of your code.    # currently, this file can be invoked with:    # python cfr.py <path/to/gamefile> <num PureCFR/MCCFR iterations>    filename = sys.argv[1]    iterations = int(sys.argv[2])    game = Game()    game.read_game_file(filename)    strategy_profile = solve_game(game, iterations)    #print "Expected Value: " + str(game.compute_strategy_profile_ev(strategy_profile))    print "Exploitability: " + str(game.compute_strategy_profile_exp(strategy_profile))